{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 1])\n",
      "torch.Size([4, 4, 4]) torch.Size([4, 4, 4]) torch.Size([4, 4, 4]) torch.Size([4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# About word embedding, taking sequence modeling as an example\n",
    "# Consider the source sentence and target sentence\n",
    "# Build a sequence, where the characters in the sequence are represented by their index in the vocabulary.\n",
    "batch_size = 2\n",
    "\n",
    "# word dict size\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "model_dim = 8\n",
    "\n",
    "# max length of source sentence and target sentence\n",
    "max_src_len = 5\n",
    "max_tgt_len = 5\n",
    "max_postion_len = 6\n",
    "\n",
    "# src_len = torch.randint(2, 5, (batch_size,))\n",
    "# tgt_len = torch.randint(2, 5, (batch_size,))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "# Create empty lists for the source sequence and target sequence.\n",
    "src_seq = []\n",
    "tgt_seq = []\n",
    "\n",
    "# step 1: create sequence\n",
    "# Generate random numbers and fill in the source sequence\n",
    "for L in src_len:\n",
    "    random_numbers = torch.randint(1, max_num_src_words, (L,))\n",
    "    padded_sequence = F.pad(random_numbers, (0, max(src_len) - L))\n",
    "    src_seq.append(padded_sequence.unsqueeze(0))\n",
    "src_seq = torch.cat(src_seq, dim=0)\n",
    "\n",
    "# Generate random numbers and fill in the target sequence\n",
    "for L in tgt_len:\n",
    "    random_numbers = torch.randint(1, max_num_tgt_words, (L,))\n",
    "    padded_sequence = F.pad(random_numbers, (0, max(tgt_len) - L))\n",
    "    tgt_seq.append(padded_sequence.unsqueeze(0))\n",
    "tgt_seq = torch.cat(tgt_seq, dim=0)\n",
    "\n",
    "# step 2: create word embedding\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "\n",
    "# step 3: create position embedding\n",
    "pos_mat = torch.arange(max_postion_len).unsqueeze(1)\n",
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).unsqueeze(0) / model_dim) \n",
    "pe_embedding_table = torch.zeros(max_postion_len, model_dim)\n",
    "\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)\n",
    "\n",
    "pe_embedding = nn.Embedding(max_postion_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "\n",
    "src_pos = [torch.arange(max(src_len)).unsqueeze(0) for _ in src_len]\n",
    "src_pos = torch.cat(src_pos, dim=0)\n",
    "tgt_pos = [torch.arange(max(tgt_len)).unsqueeze(0) for _ in tgt_len]\n",
    "tgt_pos = torch.cat(tgt_pos, dim=0)\n",
    "\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "\n",
    "# alpha1 = 0.1\n",
    "# alpha2 = 10\n",
    "# score = torch.randn(5)\n",
    "# prob1 = F.softmax(score*alpha1, dim=-1)\n",
    "# prob2 = F.softmax(score*alpha2, dim=-1)\n",
    "\n",
    "# def softmax_func(score):\n",
    "#     return F.softmax(score, dim=-1)\n",
    "\n",
    "# jaco_mat1 = torch.autograd.functional.jacobian(softmax_func, score*alpha1)\n",
    "# jaco_mat2 = torch.autograd.functional.jacobian(softmax_func, score*alpha2)\n",
    "\n",
    "# step 4 create encoder's self-attention mask\n",
    "# mask shape: (batch_size, max_src_len, max_src_len), value is 1 or -inf\n",
    "valid_encoder_pos = []\n",
    "for L in src_len:\n",
    "    valid_encoder_pos.append(F.pad(torch.ones(L), (0, max(src_len) - L)).unsqueeze(0))\n",
    "valid_encoder_pos = torch.cat(valid_encoder_pos, dim=0).unsqueeze(2)\n",
    "\n",
    "valid_decoder_pos = []\n",
    "for L in tgt_len:\n",
    "    valid_decoder_pos.append(F.pad(torch.ones(L), (0, max(tgt_len) - L)).unsqueeze(0))\n",
    "valid_decoder_pos = torch.cat(valid_decoder_pos, dim=0).unsqueeze(2)\n",
    "\n",
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "invalid_encoder_pos_matrix = 1 - valid_encoder_pos_matrix\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "\n",
    "# score = torch.randn(batch_size, max(src_len), max(src_len))\n",
    "# masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "# prob = F.softmax(masked_score, dim=-1)\n",
    "\n",
    "# step 5: create intra-attention mask\n",
    "# Q @ K^T shape: [batch_size, tgt_seq_len, src_seq_len]\n",
    "valid_cross_pos = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "invalid_cross_pos = 1 - valid_cross_pos\n",
    "mask_cross_attention = invalid_cross_pos.to(torch.bool)\n",
    "\n",
    "# step 6: decoder self-attention mask\n",
    "valid_decoder_tri_matrix = []\n",
    "for L in tgt_len:\n",
    "    tri_matrix = torch.tril(torch.ones(L, L))\n",
    "    valid_decoder_tri_matrix.append(F.pad(tri_matrix, (0, max(tgt_len) - L, 0, max(tgt_len) - L)).unsqueeze(0))\n",
    "\n",
    "valid_decoder_tri_matrix = torch.cat(valid_decoder_tri_matrix, dim=0)\n",
    "invalid_decoder_tri_matrix = 1 - valid_decoder_tri_matrix\n",
    "mask_decoder_self_attention = invalid_decoder_tri_matrix.to(torch.bool)\n",
    "\n",
    "# score = torch.randn(batch_size, max(tgt_len), max(tgt_len))\n",
    "# masked_score = score.masked_fill(mask_decoder_self_attention, -1e9)\n",
    "# prob = F.softmax(masked_score, dim=-1)\n",
    "\n",
    "# step 7: create scaled self-attention\n",
    "def scaled_dot_product_attention(Q, K, V, atten_mask):\n",
    "    # shape of Q, K, V: [batch_size*num_head, seq_len, model_dim/num_head]\n",
    "    print(Q.shape, K.shape, V.shape, atten_mask.shape)\n",
    "    score = torch.bmm(Q, K.transpose(-2, -1)) / np.sqrt(model_dim)\n",
    "    masked_score = score.masked_fill(atten_mask, -1e9)\n",
    "    prob = F.softmax(masked_score, dim=-1)\n",
    "    context = torch.bmm(prob, V)\n",
    "    return context\n",
    "\n",
    "num_head = 2\n",
    "# evaluate scaled_dot_product_attention\n",
    "Q = torch.randn(batch_size*num_head, max(tgt_len), model_dim//num_head)\n",
    "K = torch.randn(batch_size*num_head, max(src_len), model_dim//num_head)\n",
    "V = torch.randn(batch_size*num_head, max(src_len), model_dim//num_head)\n",
    "atten_mask = mask_encoder_self_attention.repeat(num_head, 1, 1)\n",
    "\n",
    "context = scaled_dot_product_attention(Q, K, V, atten_mask)\n",
    "\n",
    "display(\n",
    "    # src_embedding,\n",
    "    # tgt_embedding,\n",
    "    # atten_mask,\n",
    "    # context\n",
    "    # mask_decoder_self_attention,\n",
    "    # masked_score,\n",
    "    # prob\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7980,  0.7009,  0.2552],\n",
       "         [-1.1071,  2.1099,  0.1299],\n",
       "         [ 2.3306, -1.1239,  1.3476],\n",
       "         [-1.0456,  0.7607,  0.5408]],\n",
       "\n",
       "        [[-0.9478, -0.0352,  0.8931],\n",
       "         [ 0.2021,  0.4964, -1.4541],\n",
       "         [-0.3507, -0.8405,  1.1875],\n",
       "         [ 0.5450,  0.4047, -0.2995]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    2, -100],\n",
       "        [   2,    0,    3]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.6864, 3.6677, 0.0000],\n",
       "        [1.7471, 1.5478, 2.2011]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# step 8: mask loss\n",
    "\n",
    "logits = torch.randn(2, 3, 4).transpose(1, 2)\n",
    "label = torch.randint(0, 4, (2, 3))\n",
    "\n",
    "tgt_len = torch.tensor([2, 3]).to(torch.int32)\n",
    "\n",
    "tgt_mask = []\n",
    "for L in tgt_len:\n",
    "    tgt_mask.append(F.pad(torch.ones(L), (0, max(tgt_len) - L)).unsqueeze(0))\n",
    "tgt_mask = torch.cat(tgt_mask, dim=0)\n",
    "\n",
    "label[0, 2] = -100\n",
    "\n",
    "display(\n",
    "    tgt_mask,\n",
    "    logits,\n",
    "    label,\n",
    "    F.cross_entropy(logits, label, reduction='none')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
