{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\zyf13\\Desktop\\test\\Transformer-Learning\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import copy\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "    \n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>']) \n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2049990]) torch.Size([214417]) torch.Size([241859])\n"
          ]
        }
      ],
      "source": [
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "print(train_data.shape, val_data.shape, test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 100  # embedding dimension\n",
        "d_hid = 100  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 1  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2  # dropout probability\n",
        "\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n",
        "\n",
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target\n",
        "\n",
        "def train(model: nn.Module, epoch) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        seq_len = data.size(0)\n",
        "\n",
        "        if seq_len != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:seq_len, :seq_len]\n",
        "\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            if seq_len != bptt:\n",
        "                src_mask = src_mask[:seq_len, :seq_len]\n",
        "\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model params to C:\\Users\\zyf13\\AppData\\Local\\Temp\\tmp9sadl_8g\n",
            "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 19.08 | loss  7.88 | ppl  2632.18\n",
            "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch  9.50 | loss  6.82 | ppl   917.96\n",
            "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch  9.24 | loss  6.43 | ppl   618.88\n",
            "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch  9.49 | loss  6.31 | ppl   551.58\n",
            "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch  9.33 | loss  6.19 | ppl   489.44\n",
            "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch  9.21 | loss  6.17 | ppl   480.40\n",
            "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch  9.40 | loss  6.13 | ppl   458.87\n",
            "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch  9.47 | loss  6.11 | ppl   448.93\n",
            "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch  9.24 | loss  6.03 | ppl   417.64\n",
            "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch  9.39 | loss  6.03 | ppl   414.69\n",
            "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch  9.13 | loss  5.91 | ppl   368.38\n",
            "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch  9.94 | loss  5.97 | ppl   393.13\n",
            "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch  9.29 | loss  5.96 | ppl   388.32\n",
            "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch  9.19 | loss  5.90 | ppl   366.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 31.54s | valid loss  5.82 | valid ppl   338.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 11.13 | loss  5.88 | ppl   356.59\n",
            "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 10.05 | loss  5.85 | ppl   348.59\n",
            "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch  9.59 | loss  5.70 | ppl   298.54\n",
            "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch  9.31 | loss  5.73 | ppl   307.32\n",
            "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch  9.64 | loss  5.69 | ppl   294.52\n",
            "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch  9.71 | loss  5.71 | ppl   301.68\n",
            "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch  9.39 | loss  5.72 | ppl   303.79\n",
            "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch  9.36 | loss  5.73 | ppl   307.23\n",
            "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch  9.34 | loss  5.68 | ppl   293.04\n",
            "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch  9.98 | loss  5.70 | ppl   297.65\n",
            "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch  9.47 | loss  5.59 | ppl   266.87\n",
            "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch  9.75 | loss  5.68 | ppl   292.27\n",
            "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch  9.62 | loss  5.68 | ppl   291.73\n",
            "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch  9.89 | loss  5.63 | ppl   277.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 30.11s | valid loss  5.72 | valid ppl   306.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch  9.12 | loss  5.64 | ppl   282.14\n",
            "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch  9.17 | loss  5.64 | ppl   282.75\n",
            "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch  9.12 | loss  5.49 | ppl   241.39\n",
            "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch  9.32 | loss  5.54 | ppl   254.50\n",
            "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch  9.23 | loss  5.50 | ppl   243.70\n",
            "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch  9.45 | loss  5.52 | ppl   250.79\n",
            "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch  9.12 | loss  5.54 | ppl   254.92\n",
            "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch  9.57 | loss  5.55 | ppl   258.16\n",
            "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch  9.83 | loss  5.52 | ppl   248.45\n",
            "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch  9.50 | loss  5.53 | ppl   252.56\n",
            "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch  9.38 | loss  5.41 | ppl   223.67\n",
            "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch  9.47 | loss  5.51 | ppl   247.79\n",
            "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch  9.91 | loss  5.52 | ppl   248.61\n",
            "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch  9.45 | loss  5.48 | ppl   238.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 28.99s | valid loss  5.71 | valid ppl   301.69\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "with TemporaryDirectory() as tempdir:\n",
        "    print(f\"Saving model params to {tempdir}\")\n",
        "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(model, epoch)\n",
        "        val_loss = evaluate(model, val_data)\n",
        "        val_ppl = math.exp(val_loss)\n",
        "        elapsed = time.time() - epoch_start_time\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "        print('-' * 89)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "        scheduler.step()\n",
        "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the best model on the test dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.62 | test ppl   276.60\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "088bdccf4e4a0afd517ed1ded3a7c5b5a1b246ecfb9b545e1af2ed437848034b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
